{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved BERT Fine-tuning for Text Classification\n",
    "\n",
    "## Improvements:\n",
    "- ✅ Type hints for better code clarity\n",
    "- ✅ Comprehensive docstrings\n",
    "- ✅ Configuration class for hyperparameters\n",
    "- ✅ Early stopping implementation\n",
    "- ✅ Better error handling\n",
    "- ✅ Modular code structure\n",
    "- ✅ Logging system\n",
    "- ✅ Constants management\n",
    "- ✅ Best practices following PEP 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install pandas transformers torch scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BertConfig:\n",
    "    \"\"\"Configuration class for BERT fine-tuning hyperparameters.\"\"\"\n",
    "    \n",
    "    # Model configuration\n",
    "    model_name: str = 'bert-base-uncased'\n",
    "    num_classes: int = 2\n",
    "    bert_hidden_size: int = 768\n",
    "    hidden_size: int = 256\n",
    "    dropout_rate: float = 0.3\n",
    "    dropout_rate_2: float = 0.2\n",
    "    \n",
    "    # Training configuration\n",
    "    max_seq_length: int = 256\n",
    "    batch_size: int = 16\n",
    "    num_epochs: int = 4\n",
    "    learning_rate: float = 2e-5\n",
    "    weight_decay: float = 0.01\n",
    "    warmup_ratio: float = 0.1\n",
    "    max_grad_norm: float = 1.0\n",
    "    \n",
    "    # Data configuration\n",
    "    data_path: str = './data/IMDB Dataset Train.csv'\n",
    "    test_size: float = 0.2\n",
    "    random_state: int = 42\n",
    "    \n",
    "    # Early stopping configuration\n",
    "    early_stopping_patience: int = 3\n",
    "    early_stopping_delta: float = 0.001\n",
    "    \n",
    "    # Model saving\n",
    "    model_save_path: str = './finetuned_bert.pth'\n",
    "    \n",
    "    # Device configuration\n",
    "    device: Optional[str] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Post initialization to set device.\"\"\"\n",
    "        if self.device is None:\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        logger.info(f\"Using device: {self.device}\")\n",
    "    \n",
    "    def display(self) -> None:\n",
    "        \"\"\"Display configuration parameters.\"\"\"\n",
    "        logger.info(\"Configuration:\")\n",
    "        for key, value in self.__dict__.items():\n",
    "            logger.info(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_imdb_dataset(data_dir: str = './data') -> str:\n",
    "    \"\"\"Download IMDB dataset from GitHub.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory to save the dataset\n",
    "        \n",
    "    Returns:\n",
    "        Path to the downloaded dataset\n",
    "        \n",
    "    Raises:\n",
    "        RuntimeError: If download fails\n",
    "    \"\"\"\n",
    "    data_path = Path(data_dir)\n",
    "    data_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    file_path = data_path / 'IMDB Dataset Train.csv'\n",
    "    \n",
    "    if file_path.exists():\n",
    "        logger.info(f\"Dataset already exists at {file_path}\")\n",
    "        return str(file_path)\n",
    "    \n",
    "    logger.info(\"Downloading IMDB dataset...\")\n",
    "    url = \"https://raw.githubusercontent.com/Ankit152/IMDB-sentiment-analysis/master/IMDB-Dataset.csv\"\n",
    "    \n",
    "    import urllib.request\n",
    "    try:\n",
    "        urllib.request.urlretrieve(url, file_path)\n",
    "        logger.info(f\"Dataset downloaded successfully to {file_path}\")\n",
    "        return str(file_path)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to download dataset: {e}\")\n",
    "\n",
    "\n",
    "def load_imdb_data(file_path: str) -> Tuple[List[str], List[int]]:\n",
    "    \"\"\"Load IMDB dataset from CSV file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the CSV file\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (texts, labels)\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If file doesn't exist\n",
    "        ValueError: If required columns are missing\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"The file '{file_path}' does not exist.\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        if 'review' not in df.columns or 'sentiment' not in df.columns:\n",
    "            raise ValueError(\"CSV must contain 'review' and 'sentiment' columns\")\n",
    "        \n",
    "        texts = df['review'].tolist()\n",
    "        labels = [1 if sentiment == \"positive\" else 0 for sentiment in df['sentiment'].tolist()]\n",
    "        \n",
    "        logger.info(f\"Loaded {len(texts)} samples from {file_path}\")\n",
    "        return texts, labels\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error loading data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImdbDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for IMDB text classification.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text samples\n",
    "        labels: List of labels\n",
    "        tokenizer: BERT tokenizer\n",
    "        max_seq_length: Maximum sequence length for padding/truncation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        labels: List[int],\n",
    "        tokenizer: BertTokenizer,\n",
    "        max_seq_length: int\n",
    "    ):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return the number of samples.\"\"\"\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Get a single sample.\n",
    "        \n",
    "        Args:\n",
    "            idx: Sample index\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing input_ids, attention_mask, and label tensors\n",
    "        \"\"\"\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_seq_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved BERT Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"BERT-based text classifier with custom head.\n",
    "    \n",
    "    Args:\n",
    "        config: Configuration object containing model parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: BertConfig):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Load pretrained BERT\n",
    "        self.bert = BertModel.from_pretrained(config.model_name)\n",
    "        \n",
    "        # Custom classification head\n",
    "        self.dropout1 = nn.Dropout(config.dropout_rate)\n",
    "        self.fc1 = nn.Linear(config.bert_hidden_size, config.hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(config.dropout_rate_2)\n",
    "        self.fc2 = nn.Linear(config.hidden_size, config.num_classes)\n",
    "        \n",
    "        logger.info(f\"Initialized BertClassifier with {config.model_name}\")\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Token IDs tensor of shape (batch_size, seq_length)\n",
    "            attention_mask: Attention mask tensor of shape (batch_size, seq_length)\n",
    "            \n",
    "        Returns:\n",
    "            Logits tensor of shape (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        # Get BERT output\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        \n",
    "        # Classification head\n",
    "        x = self.dropout1(pooled_output)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        logits = self.fc2(x)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def get_num_parameters(self) -> int:\n",
    "        \"\"\"Get total number of trainable parameters.\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping utility to stop training when validation metric stops improving.\n",
    "    \n",
    "    Args:\n",
    "        patience: Number of epochs to wait before stopping\n",
    "        delta: Minimum change to qualify as improvement\n",
    "        mode: 'min' for loss, 'max' for accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        patience: int = 3,\n",
    "        delta: float = 0.001,\n",
    "        mode: str = 'max'\n",
    "    ):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.mode = mode\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, score: float) -> bool:\n",
    "        \"\"\"Check if training should stop.\n",
    "        \n",
    "        Args:\n",
    "            score: Current validation metric\n",
    "            \n",
    "        Returns:\n",
    "            True if model improved, False otherwise\n",
    "        \"\"\"\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            return True\n",
    "        \n",
    "        if self.mode == 'max':\n",
    "            improved = score > self.best_score + self.delta\n",
    "        else:\n",
    "            improved = score < self.best_score - self.delta\n",
    "        \n",
    "        if improved:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "            return True\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            logger.info(f\"EarlyStopping counter: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                logger.info(\"Early stopping triggered\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model: nn.Module,\n",
    "    data_loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scheduler: torch.optim.lr_scheduler._LRScheduler,\n",
    "    device: str,\n",
    "    max_grad_norm: float = 1.0\n",
    ") -> float:\n",
    "    \"\"\"Train model for one epoch.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        data_loader: Training data loader\n",
    "        optimizer: Optimizer\n",
    "        scheduler: Learning rate scheduler\n",
    "        device: Device to train on\n",
    "        max_grad_norm: Maximum gradient norm for clipping\n",
    "        \n",
    "    Returns:\n",
    "        Average training loss\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for batch in tqdm(data_loader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Move data to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_grad_norm)\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    model: nn.Module,\n",
    "    data_loader: DataLoader,\n",
    "    device: str\n",
    ") -> Tuple[float, str, np.ndarray]:\n",
    "    \"\"\"Evaluate model on validation/test data.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        data_loader: Evaluation data loader\n",
    "        device: Device to evaluate on\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (accuracy, classification_report, confusion_matrix)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actual_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            \n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            actual_labels.extend(labels.cpu().tolist())\n",
    "    \n",
    "    accuracy = accuracy_score(actual_labels, predictions)\n",
    "    report = classification_report(actual_labels, predictions)\n",
    "    conf_matrix = confusion_matrix(actual_labels, predictions)\n",
    "    \n",
    "    return accuracy, report, conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scheduler: torch.optim.lr_scheduler._LRScheduler,\n",
    "    config: BertConfig,\n",
    "    early_stopping: Optional[EarlyStopping] = None\n",
    ") -> Dict[str, List[float]]:\n",
    "    \"\"\"Complete training loop with validation and early stopping.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        train_loader: Training data loader\n",
    "        val_loader: Validation data loader\n",
    "        optimizer: Optimizer\n",
    "        scheduler: Learning rate scheduler\n",
    "        config: Configuration object\n",
    "        early_stopping: Early stopping object (optional)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing training history\n",
    "    \"\"\"\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_accuracy': []\n",
    "    }\n",
    "    \n",
    "    best_accuracy = 0.0\n",
    "    \n",
    "    for epoch in range(config.num_epochs):\n",
    "        logger.info(f\"\\n{'='*50}\")\n",
    "        logger.info(f\"Epoch {epoch + 1}/{config.num_epochs}\")\n",
    "        logger.info(f\"{'='*50}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss = train_epoch(\n",
    "            model, train_loader, optimizer, scheduler,\n",
    "            config.device, config.max_grad_norm\n",
    "        )\n",
    "        logger.info(f\"Average Training Loss: {train_loss:.4f}\")\n",
    "        \n",
    "        # Evaluate\n",
    "        accuracy, report, conf_matrix = evaluate(model, val_loader, config.device)\n",
    "        logger.info(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "        logger.info(f\"\\n{report}\")\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_accuracy'].append(accuracy)\n",
    "        \n",
    "        # Save best model\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(model.state_dict(), config.model_save_path)\n",
    "            logger.info(f\"Model saved with accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if early_stopping is not None:\n",
    "            early_stopping(accuracy)\n",
    "            if early_stopping.early_stop:\n",
    "                logger.info(\"Early stopping triggered, ending training\")\n",
    "                break\n",
    "    \n",
    "    logger.info(f\"\\n{'='*50}\")\n",
    "    logger.info(f\"Best Validation Accuracy: {best_accuracy:.4f}\")\n",
    "    logger.info(f\"{'='*50}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize configuration\n",
    "config = BertConfig()\n",
    "config.display()\n",
    "\n",
    "# Download and load data\n",
    "try:\n",
    "    data_path = download_imdb_dataset()\n",
    "    texts, labels = load_imdb_data(data_path)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load data: {e}\")\n",
    "    raise\n",
    "\n",
    "# Split data\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    texts, labels,\n",
    "    test_size=config.test_size,\n",
    "    random_state=config.random_state\n",
    ")\n",
    "\n",
    "logger.info(f\"Training samples: {len(train_texts)}\")\n",
    "logger.info(f\"Validation samples: {len(val_texts)}\")\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(config.model_name)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ImdbDataset(train_texts, train_labels, tokenizer, config.max_seq_length)\n",
    "val_dataset = ImdbDataset(val_texts, val_labels, tokenizer, config.max_seq_length)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.batch_size)\n",
    "\n",
    "# Initialize model\n",
    "model = BertClassifier(config).to(config.device)\n",
    "logger.info(f\"Total trainable parameters: {model.get_num_parameters():,}\")\n",
    "\n",
    "# Initialize optimizer and scheduler\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config.learning_rate,\n",
    "    weight_decay=config.weight_decay\n",
    ")\n",
    "\n",
    "total_steps = len(train_loader) * config.num_epochs\n",
    "warmup_steps = int(config.warmup_ratio * total_steps)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "logger.info(f\"Total training steps: {total_steps}\")\n",
    "logger.info(f\"Warmup steps: {warmup_steps}\")\n",
    "\n",
    "# Initialize early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=config.early_stopping_patience,\n",
    "    delta=config.early_stopping_delta,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "# Train model\n",
    "try:\n",
    "    history = train(\n",
    "        model, train_loader, val_loader,\n",
    "        optimizer, scheduler, config, early_stopping\n",
    "    )\n",
    "    logger.info(\"Training completed successfully!\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Training failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training History (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(history: Dict[str, List[float]]) -> None:\n",
    "    \"\"\"Plot training history.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot loss\n",
    "    ax1.plot(history['train_loss'], marker='o')\n",
    "    ax1.set_title('Training Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax2.plot(history['val_accuracy'], marker='o', color='green')\n",
    "    ax2.set_title('Validation Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Uncomment to plot\n",
    "# plot_training_history(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
