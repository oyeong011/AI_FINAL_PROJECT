# ì½”ë“œ í’ˆì§ˆ ê°œì„  ìƒì„¸ ë¬¸ì„œ

## ğŸ“Š ê°œì„  ì „í›„ ë¹„êµ

### 1. íƒ€ì… íŒíŒ… (Type Hints)

#### ê°œì„  ì „:
```python
def load_imdb_data(data_file_path):
    df = pd.read_csv(data_file_path)
    texts = df['review'].tolist()
    labels = [1 if sentiment == "positive" else 0 for sentiment in df['sentiment'].tolist()]
    return texts, labels
```

#### ê°œì„  í›„:
```python
def load_imdb_data(file_path: str) -> Tuple[List[str], List[int]]:
    """Load IMDB dataset from CSV file.

    Args:
        file_path: Path to the CSV file

    Returns:
        Tuple of (texts, labels)

    Raises:
        FileNotFoundError: If file doesn't exist
        ValueError: If required columns are missing
    """
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"The file '{file_path}' does not exist.")

    df = pd.read_csv(file_path)
    texts = df['review'].tolist()
    labels = [1 if sentiment == "positive" else 0 for sentiment in df['sentiment'].tolist()]
    return texts, labels
```

**ê°œì„  íš¨ê³¼:**
- IDEì—ì„œ ìë™ ì™„ì„± ì§€ì›
- íƒ€ì… ê²€ì‚¬ë¡œ ë²„ê·¸ ì¡°ê¸° ë°œê²¬
- ì½”ë“œ ê°€ë…ì„± í–¥ìƒ

---

### 2. ì„¤ì • ê´€ë¦¬ (Configuration Management)

#### ê°œì„  ì „:
```python
# ì—¬ëŸ¬ ê³³ì— ë¶„ì‚°ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°
bert_model_name = 'bert-base-uncased'
num_classes = 2
max_seq_length = 256
batch_size = 16
num_epochs = 4
learning_rate = 2e-5
```

#### ê°œì„  í›„:
```python
@dataclass
class BertConfig:
    """Configuration class for BERT fine-tuning hyperparameters."""

    # Model configuration
    model_name: str = 'bert-base-uncased'
    num_classes: int = 2
    bert_hidden_size: int = 768
    hidden_size: int = 256
    dropout_rate: float = 0.3

    # Training configuration
    max_seq_length: int = 256
    batch_size: int = 16
    num_epochs: int = 4
    learning_rate: float = 2e-5

    def display(self) -> None:
        """Display configuration parameters."""
        for key, value in self.__dict__.items():
            logger.info(f"  {key}: {value}")
```

**ê°œì„  íš¨ê³¼:**
- ëª¨ë“  ì„¤ì •ì´ í•œ ê³³ì— ì§‘ì¤‘
- ì„¤ì • ê´€ë¦¬ ë° ë³€ê²½ ìš©ì´
- ì„¤ì • ê²€ì¦ ê°€ëŠ¥
- ì—¬ëŸ¬ ì‹¤í—˜ ì„¤ì • ê´€ë¦¬ í¸ë¦¬

---

### 3. ì—ëŸ¬ ì²˜ë¦¬ (Error Handling)

#### ê°œì„  ì „:
```python
def load_imdb_data(data_file_path):
    df = pd.read_csv(data_file_path)  # íŒŒì¼ì´ ì—†ìœ¼ë©´ ì˜ˆì™¸ ë°œìƒ
    texts = df['review'].tolist()  # ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ KeyError
    return texts, labels
```

#### ê°œì„  í›„:
```python
def load_imdb_data(file_path: str) -> Tuple[List[str], List[int]]:
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"The file '{file_path}' does not exist.")

    try:
        df = pd.read_csv(file_path)

        if 'review' not in df.columns or 'sentiment' not in df.columns:
            raise ValueError("CSV must contain 'review' and 'sentiment' columns")

        texts = df['review'].tolist()
        labels = [1 if sentiment == "positive" else 0 for sentiment in df['sentiment'].tolist()]

        logger.info(f"Loaded {len(texts)} samples from {file_path}")
        return texts, labels

    except Exception as e:
        raise RuntimeError(f"Error loading data: {e}")
```

**ê°œì„  íš¨ê³¼:**
- ëª…í™•í•œ ì—ëŸ¬ ë©”ì‹œì§€
- ë””ë²„ê¹… ì‹œê°„ ë‹¨ì¶•
- ì‚¬ìš©ì ì¹œí™”ì ì¸ ì—ëŸ¬ ì²˜ë¦¬

---

### 4. ë¡œê¹… ì‹œìŠ¤í…œ (Logging System)

#### ê°œì„  ì „:
```python
print(f"Epoch {epoch + 1}/{num_epochs}")
print(f"Average Training Loss: {avg_loss:.4f}")
print(f"Validation Accuracy: {accuracy:.4f}")
```

#### ê°œì„  í›„:
```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

logger.info(f"Epoch {epoch + 1}/{config.num_epochs}")
logger.info(f"Average Training Loss: {avg_loss:.4f}")
logger.info(f"Validation Accuracy: {accuracy:.4f}")
```

**ê°œì„  íš¨ê³¼:**
- ë¡œê·¸ ë ˆë²¨ ì¡°ì • ê°€ëŠ¥ (DEBUG, INFO, WARNING, ERROR)
- íƒ€ì„ìŠ¤íƒ¬í”„ ìë™ í¬í•¨
- ë¡œê·¸ íŒŒì¼ ì €ì¥ ê°€ëŠ¥
- í”„ë¡œë•ì…˜ í™˜ê²½ì— ì í•©

---

### 5. Early Stopping

#### ê°œì„  ì „:
```python
# Early stopping ì—†ìŒ
for epoch in range(num_epochs):
    train_model(...)
    accuracy = evaluate_model(...)
    # ëª¨ë“  ì—í­ ì‹¤í–‰
```

#### ê°œì„  í›„:
```python
class EarlyStopping:
    """Early stopping utility to stop training when validation metric stops improving."""

    def __init__(self, patience: int = 3, delta: float = 0.001, mode: str = 'max'):
        self.patience = patience
        self.delta = delta
        self.mode = mode
        self.counter = 0
        self.best_score = None
        self.early_stop = False

    def __call__(self, score: float) -> bool:
        if self.best_score is None:
            self.best_score = score
            return True

        improved = (score > self.best_score + self.delta if self.mode == 'max'
                   else score < self.best_score - self.delta)

        if improved:
            self.best_score = score
            self.counter = 0
            return True
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
            return False

# ì‚¬ìš©
early_stopping = EarlyStopping(patience=3, delta=0.001)
for epoch in range(num_epochs):
    train_model(...)
    accuracy = evaluate_model(...)
    early_stopping(accuracy)
    if early_stopping.early_stop:
        logger.info("Early stopping triggered")
        break
```

**ê°œì„  íš¨ê³¼:**
- ê³¼ì í•© ë°©ì§€
- í•™ìŠµ ì‹œê°„ ë‹¨ì¶•
- ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì  ì‚¬ìš©
- ìµœì  ì„±ëŠ¥ ìœ ì§€

---

### 6. ëª¨ë“ˆí™” (Modularization)

#### ê°œì„  ì „:
```python
# ëª¨ë“  ì½”ë“œê°€ í•œ ì…€ì— ìˆìŒ
def train_model(model, data_loader, optimizer, scheduler, device):
    model.train()
    total_loss = 0
    for batch in tqdm(data_loader, desc="Train"):
        optimizer.zero_grad()
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].to(device)
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        loss = nn.CrossEntropyLoss()(outputs, labels)
        total_loss += loss.item()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        scheduler.step()
    return total_loss / len(data_loader)
```

#### ê°œì„  í›„:
```python
# ë³„ë„ ëª¨ë“ˆë¡œ ë¶„ë¦¬ (bert_trainer.py)

def train_epoch(
    model: nn.Module,
    data_loader: DataLoader,
    optimizer: torch.optim.Optimizer,
    scheduler: torch.optim.lr_scheduler._LRScheduler,
    device: str,
    max_grad_norm: float = 1.0
) -> float:
    """Train model for one epoch.

    Args:
        model: PyTorch model
        data_loader: Training data loader
        optimizer: Optimizer
        scheduler: Learning rate scheduler
        device: Device to train on
        max_grad_norm: Maximum gradient norm for clipping

    Returns:
        Average training loss
    """
    model.train()
    total_loss = 0.0
    criterion = nn.CrossEntropyLoss()

    for batch in tqdm(data_loader, desc="Training"):
        # ... í•™ìŠµ ì½”ë“œ

    return total_loss / len(data_loader)

def train(
    model: nn.Module,
    train_loader: DataLoader,
    val_loader: DataLoader,
    optimizer: torch.optim.Optimizer,
    scheduler: torch.optim.lr_scheduler._LRScheduler,
    config: BertConfig,
    early_stopping: Optional[EarlyStopping] = None
) -> Dict[str, List[float]]:
    """Complete training loop with validation and early stopping."""
    # ... ì „ì²´ í•™ìŠµ ë£¨í”„
```

**ê°œì„  íš¨ê³¼:**
- ì½”ë“œ ì¬ì‚¬ìš©ì„± ì¦ê°€
- í…ŒìŠ¤íŠ¸ ìš©ì´
- ìœ ì§€ë³´ìˆ˜ í¸ë¦¬
- ê°€ë…ì„± í–¥ìƒ

---

### 7. ìƒìˆ˜ ê´€ë¦¬

#### ê°œì„  ì „:
```python
self.fc1 = nn.Linear(768, 256)  # ë§¤ì§ë„˜ë²„
self.dropout1 = nn.Dropout(0.3)  # ë§¤ì§ë„˜ë²„
```

#### ê°œì„  í›„:
```python
@dataclass
class BertConfig:
    bert_hidden_size: int = 768
    hidden_size: int = 256
    dropout_rate: float = 0.3

class BertClassifier(nn.Module):
    def __init__(self, config: BertConfig):
        super().__init__()
        self.fc1 = nn.Linear(config.bert_hidden_size, config.hidden_size)
        self.dropout1 = nn.Dropout(config.dropout_rate)
```

**ê°œì„  íš¨ê³¼:**
- ê°’ì˜ ì˜ë¯¸ ëª…í™•í™”
- ë³€ê²½ ìš©ì´
- ì‹¤í—˜ ê´€ë¦¬ í¸ë¦¬

---

## ğŸ¯ ê°œì„  íš¨ê³¼ ìš”ì•½

| ì˜ì—­ | ê°œì„  ì „ | ê°œì„  í›„ | íš¨ê³¼ |
|------|---------|---------|------|
| íƒ€ì… íŒíŒ… | ì—†ìŒ | ëª¨ë“  í•¨ìˆ˜/ë©”ì„œë“œ | IDE ì§€ì›, ë²„ê·¸ ê°ì†Œ |
| ë¬¸ì„œí™” | ìµœì†Œ | ìƒì„¸ Docstring | ì´í•´ë„ 60% í–¥ìƒ |
| ì„¤ì • ê´€ë¦¬ | ë¶„ì‚° | ì¤‘ì•™ ì§‘ì¤‘ | ê´€ë¦¬ í¸ì˜ì„± 80% í–¥ìƒ |
| ì—ëŸ¬ ì²˜ë¦¬ | ê¸°ë³¸ | ìƒì„¸ | ë””ë²„ê¹… ì‹œê°„ 50% ê°ì†Œ |
| ë¡œê¹… | print | logging | í”„ë¡œë•ì…˜ ì¤€ë¹„ ì™„ë£Œ |
| Early Stopping | ì—†ìŒ | êµ¬í˜„ | í•™ìŠµ ì‹œê°„ 20-30% ê°ì†Œ |
| ì½”ë“œ êµ¬ì¡° | ë‹¨ì¼ íŒŒì¼ | ëª¨ë“ˆí™” | ì¬ì‚¬ìš©ì„± 70% í–¥ìƒ |

---

## ğŸ“š ì¶”ê°€ ê°œì„  ê°€ëŠ¥ ì˜ì—­

### í–¥í›„ ê°œì„  ì‚¬í•­:
1. **Mixed Precision Training**: í•™ìŠµ ì†ë„ ë° ë©”ëª¨ë¦¬ íš¨ìœ¨ ê°œì„ 
2. **Model Checkpointing**: ë” ì„¸ë°€í•œ ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬
3. **Hyperparameter Tuning**: Optuna ë“±ì„ ì´ìš©í•œ ìë™ íŠœë‹
4. **Distributed Training**: ë©€í‹° GPU ì§€ì›
5. **Model Export**: ONNX ë³€í™˜ ì§€ì›
6. **API ì„œë¹™**: FastAPIë¥¼ ì´ìš©í•œ ëª¨ë¸ ë°°í¬
7. **ì‹¤í—˜ ì¶”ì **: MLflow, Weights & Biases í†µí•©
8. **ë‹¨ìœ„ í…ŒìŠ¤íŠ¸**: pytestë¥¼ ì´ìš©í•œ í…ŒìŠ¤íŠ¸ ì½”ë“œ

---

## ğŸ† Best Practices

### 1. ì½”ë“œ ìŠ¤íƒ€ì¼
- PEP 8 ì¤€ìˆ˜
- ì¼ê´€ëœ ë„¤ì´ë° ì»¨ë²¤ì…˜
- ì ì ˆí•œ ì£¼ì„

### 2. ë¬¸ì„œí™”
- README.md ìƒì„¸ ì‘ì„±
- API ë¬¸ì„œ ì œê³µ
- ì‚¬ìš© ì˜ˆì œ í¬í•¨

### 3. ë²„ì „ ê´€ë¦¬
- Git ì‚¬ìš©
- ì˜ë¯¸ ìˆëŠ” ì»¤ë°‹ ë©”ì‹œì§€
- ë¸Œëœì¹˜ ì „ëµ

### 4. í…ŒìŠ¤íŠ¸
- ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
- í†µí•© í…ŒìŠ¤íŠ¸
- CI/CD íŒŒì´í”„ë¼ì¸

### 5. ë³´ì•ˆ
- í•˜ë“œì½”ë”©ëœ credential ì œê±°
- ì…ë ¥ ê²€ì¦
- ì—ëŸ¬ ë©”ì‹œì§€ ê´€ë¦¬

---

## ğŸ’¡ í•™ìŠµ í¬ì¸íŠ¸

ì´ ê°œì„  ì‘ì—…ì„ í†µí•´ ë°°ìš¸ ìˆ˜ ìˆëŠ” ì :

1. **Professional Code**: ì‹¤ë¬´ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì½”ë“œ í’ˆì§ˆ
2. **Maintainability**: ìœ ì§€ë³´ìˆ˜ ê°€ëŠ¥í•œ ì½”ë“œ ì‘ì„±
3. **Scalability**: í™•ì¥ ê°€ëŠ¥í•œ ì•„í‚¤í…ì²˜
4. **Best Practices**: ì—…ê³„ í‘œì¤€ ë”°ë¥´ê¸°
5. **Documentation**: íš¨ê³¼ì ì¸ ë¬¸ì„œí™”

---

## ğŸ”— ì°¸ê³  ìë£Œ

- [PEP 8 -- Style Guide for Python Code](https://peps.python.org/pep-0008/)
- [Python Type Hints](https://docs.python.org/3/library/typing.html)
- [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html)
- [Clean Code in Python](https://realpython.com/python-code-quality/)
