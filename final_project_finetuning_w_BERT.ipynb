{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oyeong011/AI_FINAL_PROJECT/blob/main/final_project_finetuning_w_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xbJWUcba4vw"
      },
      "source": [
        "# Artificial intelligence Final Project: Finetuning_BERT\n",
        "\n",
        "Copyright (C) Computer Science & Engineering, Soongsil University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them (November 2025.)\n",
        "\n",
        "BERT(Bidirectional Encoder Representations from Transformers) is a groundbreaking model in the NLP domain. This tutorial provides a step-by-step guide on how to fine-tune the lightweight BERT variant using Hugging Face's transformers library for text classification tasks.<br>\n",
        "\n",
        "This is about BERT (Devlin et al., 2018).<br>\n",
        "https://arxiv.org/abs/1810.04805\n",
        "\n",
        "The code below are based on the following link. <br>\n",
        "https://medium.com/@khang.pham.exxact/text-classification-with-bert-7afaacc5e49b\n",
        "\n",
        "\n",
        "### Fine-tune the model\n",
        "1. Design your model's prediction head\n",
        "2. Finetune the model by changing the hyperparameters.\n",
        "3. You will get a score based on the your (hidden) test accuracy for text classification (ranking-based).  \n",
        "\n",
        "### Submitting your work:\n",
        "<font color=red>**DO NOT clear the final outputs**</font> so that we can grade both your code and results.  \n",
        "\n",
        "\n",
        "Now proceed to the code.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CYpw485a4vy"
      },
      "source": [
        "## Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8h3wH_ca4vy"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FX34X1Eja4vz",
        "outputId": "bb9bd141-2e23-4de2-de10-efa7fa9f155e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mTraceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/commands/install.py\", line 447, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/commands/install.py\", line 578, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/operations/check.py\", line 42, in create_package_set_from_installed\n",
            "    dependencies = list(dist.iter_dependencies())\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/metadata/importlib/_dists.py\", line 222, in iter_dependencies\n",
            "    req = Requirement(req_string.strip())\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/packaging/requirements.py\", line 36, in __init__\n",
            "    parsed = _parse_requirement(requirement_string)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/packaging/_parser.py\", line 62, in parse_requirement\n",
            "    return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/packaging/_parser.py\", line 80, in _parse_requirement\n",
            "    url, specifier, marker = _parse_requirement_details(tokenizer)\n",
            "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/packaging/_parser.py\", line 124, in _parse_requirement_details\n",
            "    marker = _parse_requirement_marker(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/packaging/_parser.py\", line 151, in _parse_requirement_marker\n",
            "    marker = _parse_marker(tokenizer)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/packaging/_parser.py\", line 266, in _parse_marker\n",
            "    expression = [_parse_marker_atom(tokenizer)]\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/packaging/_parser.py\", line 291, in _parse_marker_atom\n",
            "    marker = _parse_marker_item(tokenizer)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/packaging/_parser.py\", line 301, in _parse_marker_item\n",
            "    marker_var_left = _parse_marker_var(tokenizer)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/packaging/_parser.py\", line 314, in _parse_marker_var\n",
            "    if tokenizer.check(\"VARIABLE\"):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/packaging/_tokenizer.py\", line 129, in check\n",
            "    match = expression.match(self.source, self.position)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/__main__.py\", line 24, in <module>\n",
            "    sys.exit(_main())\n",
            "             ^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n",
            "    return command.main(cmd_args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
            "    return self._main(args)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n",
            "    return run(options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 216, in exc_logging_wrapper\n",
            "    logger.debug(\"Exception information:\", exc_info=True)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1527, in debug\n",
            "    self._log(DEBUG, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1684, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1700, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1762, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1028, in handle\n",
            "    self.emit(record)\n",
            "  File \"/usr/lib/python3.12/logging/handlers.py\", line 75, in emit\n",
            "    logging.FileHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1280, in emit\n",
            "    StreamHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1160, in emit\n",
            "    msg = self.format(record)\n",
            "          ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 999, in format\n",
            "    return fmt.format(record)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/utils/logging.py\", line 112, in format\n",
            "    formatted = super().format(record)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 711, in format\n",
            "    record.exc_text = self.formatException(record.exc_info)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 661, in formatException\n",
            "    traceback.print_exception(ei[0], ei[1], tb, None, sio)\n",
            "  File \"/usr/lib/python3.12/traceback.py\", line 124, in print_exception\n",
            "    te = TracebackException(type(value), value, tb, limit=limit, compact=True)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/traceback.py\", line 733, in __init__\n",
            "    self.stack = StackSummary._extract_from_extended_frame_gen(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/traceback.py\", line 438, in _extract_from_extended_frame_gen\n",
            "    f.line\n",
            "  File \"/usr/lib/python3.12/traceback.py\", line 323, in line\n",
            "    self._line = linecache.getline(self.filename, self.lineno)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/linecache.py\", line 30, in getline\n",
            "    lines = getlines(filename, module_globals)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/linecache.py\", line 46, in getlines\n",
            "    return updatecache(filename, module_globals)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/linecache.py\", line 141, in updatecache\n",
            "    with tokenize.open(fullname) as fp:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/tokenize.py\", line 459, in open\n",
            "    encoding, lines = detect_encoding(buffer.readline)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/tokenize.py\", line 428, in detect_encoding\n",
            "    first = read_or_stop()\n",
            "            ^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/tokenize.py\", line 386, in read_or_stop\n",
            "    return readline()\n",
            "           ^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pip install pandas\n",
        "!python3 -m pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly2F37uUa4v0"
      },
      "source": [
        "### import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmPSq4pma4v0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "# from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKE7enHGa4v0"
      },
      "source": [
        "### Specify your GPU number if necessary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eemX8exxa4v0",
        "outputId": "615a741c-d789-45fa-b022-7229550e5611"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: CUDA_VISIBLE_DEVICES=0\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "%env CUDA_VISIBLE_DEVICES = 0\n",
        "\n",
        "if torch.cuda.is_available() is True:\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_q5wjA6a4v1"
      },
      "source": [
        "## Preparing dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGVK3QU0a4v1"
      },
      "source": [
        "link : https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
        "\n",
        "1. Download the dataset from attached link.\n",
        "2. Move the downloaded zip file under the \"data\" directory and then unzip the zip file.\n",
        "3. Run the following cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17_-68aea4v1",
        "outputId": "96d4f097-730c-4648-ce0b-1a1af0f01f17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-11-25 02:27:13--  https://raw.githubusercontent.com/Ankit152/IMDB-sentiment-analysis/master/IMDB-Dataset.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 66212309 (63M) [text/plain]\n",
            "Saving to: ‘./data/IMDB Dataset Train.csv’\n",
            "\n",
            "./data/IMDB Dataset 100%[===================>]  63.14M   194MB/s    in 0.3s    \n",
            "\n",
            "2025-11-25 02:27:17 (194 MB/s) - ‘./data/IMDB Dataset Train.csv’ saved [66212309/66212309]\n",
            "\n",
            "파일 다운로드 성공: ./data/IMDB Dataset Train.csv\n",
            "데이터 샘플: 50000개 로드됨\n",
            "Loaded 50000 samples\n"
          ]
        }
      ],
      "source": [
        "# 1. 기존 데이터 폴더가 있다면 삭제하고 새로 만듭니다 (충돌 방지)\n",
        "!rm -rf data\n",
        "!mkdir data\n",
        "\n",
        "# 2. 인터넷에서 IMDB 데이터셋을 다운로드합니다 (GitHub Raw URL 사용)\n",
        "# 주의: 파일명이 조금 다를 수 있으니 다운로드 후 이름을 맞춥니다.\n",
        "!wget -O \"./data/IMDB Dataset Train.csv\" \"https://raw.githubusercontent.com/Ankit152/IMDB-sentiment-analysis/master/IMDB-Dataset.csv\"\n",
        "\n",
        "# 3. 데이터 로드 확인\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "data_file_path = './data/IMDB Dataset Train.csv'\n",
        "\n",
        "if os.path.exists(data_file_path):\n",
        "    print(f\"파일 다운로드 성공: {data_file_path}\")\n",
        "    # 데이터 로드 테스트\n",
        "    df = pd.read_csv(data_file_path)\n",
        "    print(f\"데이터 샘플: {len(df)}개 로드됨\")\n",
        "else:\n",
        "    print(\"파일 다운로드 실패\")\n",
        "\n",
        "def load_imdb_data(data_file_path):\n",
        "    if os.path.exists(data_file_path):\n",
        "        df = pd.read_csv(data_file_path)\n",
        "        texts = df['review'].tolist()\n",
        "        labels = [1 if sentiment == \"positive\" else 0 for sentiment in df['sentiment'].tolist()]\n",
        "        return texts, labels\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"The file '{data_file_path}' does not exist.\")\n",
        "\n",
        "data_file_path = './data/IMDB Dataset Train.csv'\n",
        "texts, labels = load_imdb_data(data_file_path)\n",
        "print(f\"Loaded {len(texts)} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nisMnhDsa4v2"
      },
      "source": [
        "## Dataset class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAJsb207a4v2"
      },
      "outputs": [],
      "source": [
        "class CustomTextClassificationDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_seq_length):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_seq_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tcwU3w7a4v2"
      },
      "source": [
        "## Classifier head for BERT( Design your model's prediction head )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLTC4xoha4v2"
      },
      "outputs": [],
      "source": [
        "class CustomBERTClassifier(nn.Module):\n",
        "    def __init__(self, bert_model_name, num_classes):\n",
        "        super(CustomBERTClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
        "        ######################## TO-DO ########################\n",
        "        # BERT base hidden size is 768\n",
        "        self.dropout1 = nn.Dropout(0.3)\n",
        "        self.fc1 = nn.Linear(768, 256)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "        ######################## TO-DO ########################\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        ######################## TO-DO ########################\n",
        "        x = self.dropout1(pooled_output)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        logits = self.fc2(x)\n",
        "        ######################## TO-DO ########################\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0oj3__Ca4v2"
      },
      "source": [
        "## train and evaluation method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqjFg0Eza4v2"
      },
      "outputs": [],
      "source": [
        "def train_model(model, data_loader, optimizer, scheduler, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(data_loader, desc=\"Train\"):\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        ######################## TO-DO ########################\n",
        "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "        ######################## TO-DO ########################\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        # Gradient clipping to prevent exploding gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    print(f\"Average Training Loss: {avg_loss:.4f}\")\n",
        "\n",
        "def evaluate_model(model, data_loader, device):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actual_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(data_loader, desc=\"Validation\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "            predictions.extend(preds.cpu().tolist())\n",
        "            actual_labels.extend(labels.cpu().tolist())\n",
        "    return accuracy_score(actual_labels, predictions), classification_report(actual_labels, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smhFR7lUa4v2"
      },
      "source": [
        "## Hyper-parameter settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VO29t4QEa4v2",
        "outputId": "a603e84e-9b94-4eca-bae7-f0d123251add"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hyperparameters:\n",
            "  max_seq_length: 256\n",
            "  batch_size: 16\n",
            "  num_epochs: 4\n",
            "  learning_rate: 2e-05\n"
          ]
        }
      ],
      "source": [
        "# Set up parameters\n",
        "# Hint: generally, 5 ~ 10 epochs will be enough.\n",
        "bert_model_name = 'bert-base-uncased'\n",
        "num_classes = 2\n",
        "######################## TO-DO ########################\n",
        "max_seq_length = 256  # IMDB reviews can be long, 256 is a good balance\n",
        "batch_size = 16  # Adjust based on GPU memory (use 8 if OOM)\n",
        "num_epochs = 4  # BERT typically needs 2-4 epochs for fine-tuning\n",
        "learning_rate = 2e-5  # Standard BERT fine-tuning learning rate\n",
        "######################## TO-DO ########################\n",
        "\n",
        "print(f\"Hyperparameters:\")\n",
        "print(f\"  max_seq_length: {max_seq_length}\")\n",
        "print(f\"  batch_size: {batch_size}\")\n",
        "print(f\"  num_epochs: {num_epochs}\")\n",
        "print(f\"  learning_rate: {learning_rate}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11SFb-B8a4v3"
      },
      "source": [
        "## get data utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hb7Wvk8Da4v3",
        "outputId": "bef00835-3889-45cd-f1e6-2d453399b081",
        "colab": {
          "referenced_widgets": [
            "2180f000d8464e09beedd4227e45f926",
            "cbe0ac719dd243f1964de660b4a0ab34",
            "9ec39943f6724da2b0408bee668d97b2",
            "d8d991b01c4f4c918f77b37ba05eb717"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training samples: 40000\n",
            "Validation samples: 10000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2180f000d8464e09beedd4227e45f926",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cbe0ac719dd243f1964de660b4a0ab34",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ec39943f6724da2b0408bee668d97b2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d8d991b01c4f4c918f77b37ba05eb717",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "######################## DO NOT CHANGE ########################\n",
        "train_texts, val_texts, train_labels, val_labels = \\\n",
        "train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "######################## DO NOT CHANGE ########################\n",
        "\n",
        "print(f\"Training samples: {len(train_texts)}\")\n",
        "print(f\"Validation samples: {len(val_texts)}\")\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
        "train_dataset = CustomTextClassificationDataset(train_texts, train_labels, tokenizer, max_seq_length)\n",
        "val_dataset = CustomTextClassificationDataset(val_texts, val_labels, tokenizer, max_seq_length)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MH6PCCita4v3"
      },
      "source": [
        "## Define model, optimizer and scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNAM1NdSa4v3",
        "outputId": "223bbcf4-dfe0-451b-93c5-2443d84753b8",
        "colab": {
          "referenced_widgets": [
            "a5ef410b0cd14e81aa1ab7dfda39c082"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a5ef410b0cd14e81aa1ab7dfda39c082",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total training steps: 10000\n",
            "Warmup steps: 1000\n"
          ]
        }
      ],
      "source": [
        "model = CustomBERTClassifier(bert_model_name, num_classes).to(device)\n",
        "######################## TO-DO ########################\n",
        "# Use AdamW optimizer with weight decay for regularization\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "######################## TO-DO ########################\n",
        "total_steps = len(train_dataloader) * num_epochs\n",
        "# Warmup helps stabilize training in early steps\n",
        "warmup_steps = int(0.1 * total_steps)  # 10% warmup\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
        "\n",
        "print(f\"Total training steps: {total_steps}\")\n",
        "print(f\"Warmup steps: {warmup_steps}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzqG6ASma4v3"
      },
      "source": [
        "## Train model and save best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EURmW_aka4v3",
        "outputId": "e871a79f-aa58-4fe2-923d-3ce8af737817"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Epoch 1/4\n",
            "==================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100%|██████████| 2500/2500 [28:37<00:00,  1.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Training Loss: 0.3142\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 625/625 [02:52<00:00,  3.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.9080\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.94      0.91      4961\n",
            "           1       0.94      0.87      0.91      5039\n",
            "\n",
            "    accuracy                           0.91     10000\n",
            "   macro avg       0.91      0.91      0.91     10000\n",
            "weighted avg       0.91      0.91      0.91     10000\n",
            "\n",
            "Saved Trained Model.\n",
            "\n",
            "==================================================\n",
            "Epoch 2/4\n",
            "==================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100%|██████████| 2500/2500 [28:53<00:00,  1.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Training Loss: 0.1739\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 625/625 [02:51<00:00,  3.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.9207\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.94      0.92      4961\n",
            "           1       0.94      0.90      0.92      5039\n",
            "\n",
            "    accuracy                           0.92     10000\n",
            "   macro avg       0.92      0.92      0.92     10000\n",
            "weighted avg       0.92      0.92      0.92     10000\n",
            "\n",
            "Saved Trained Model.\n",
            "\n",
            "==================================================\n",
            "Epoch 3/4\n",
            "==================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100%|██████████| 2500/2500 [28:53<00:00,  1.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Training Loss: 0.1006\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 625/625 [02:51<00:00,  3.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.9235\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.89      0.92      4961\n",
            "           1       0.90      0.96      0.93      5039\n",
            "\n",
            "    accuracy                           0.92     10000\n",
            "   macro avg       0.93      0.92      0.92     10000\n",
            "weighted avg       0.93      0.92      0.92     10000\n",
            "\n",
            "Saved Trained Model.\n",
            "\n",
            "==================================================\n",
            "Epoch 4/4\n",
            "==================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100%|██████████| 2500/2500 [28:53<00:00,  1.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Training Loss: 0.0526\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 625/625 [02:53<00:00,  3.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.9270\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.92      0.93      4961\n",
            "           1       0.92      0.93      0.93      5039\n",
            "\n",
            "    accuracy                           0.93     10000\n",
            "   macro avg       0.93      0.93      0.93     10000\n",
            "weighted avg       0.93      0.93      0.93     10000\n",
            "\n",
            "Saved Trained Model.\n",
            "\n",
            "==================================================\n",
            "Best Validation Accuracy: 0.9270\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "eval_acc = 0\n",
        "for epoch in range(num_epochs):\n",
        "    model_path = './finetuned_bert.pth'\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    print(f\"{'='*50}\")\n",
        "    train_model(model, train_dataloader, optimizer, scheduler, device)\n",
        "    accuracy, report = evaluate_model(model, val_dataloader, device)\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "    print(report)\n",
        "\n",
        "    if eval_acc < accuracy:\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "        print('Saved Trained Model.')\n",
        "        eval_acc = accuracy\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"Best Validation Accuracy: {eval_acc:.4f}\")\n",
        "print(f\"{'='*50}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJuATDQ_a4v3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}